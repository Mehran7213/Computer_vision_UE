{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|First Name     |Last Name    |MtkNr.      |Due Date  | \n",
    "|:-------------:|:-----------:|:----------:|:--------:|\n",
    "| Write Here    | Write Here  |12345678     | 24.11.2025, 16:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(150,100,10)\">Computer Vision Course</h1>\n",
    "<h2 style=\"color:rgb(150,100,10)\">Assignment 2 – Feature Extraction and Segmentation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> O. Bimber, M. Abbass<br>\n",
    "<b>Date:</b> 23-06-2025\n",
    "\n",
    "<b>This file is part of the \"Computer Vision Course 2025W\" UE material.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(150,100,10)\">Table of Contents</h2>\n",
    "<ol>\n",
    "    <a style=\"color:rgb(150,100,10)\" href=\"#Feature-Extraction\"><li style=\"font-size:large;font-weight:bold\">Feature Extraction [25 points]</li></a>\n",
    "    <ol style=\"margin-bottom:15px\">\n",
    "        <a style=\"color:rgb(150,100,10)\" href=\"#Extracting-Feature-Maps-from-Pretrained-Torch-Models\"><li style=\"font-size:medium\">Extracting Feature Maps from Pretrained Torch Models</li></a>\n",
    "        <a style=\"color:rgb(150,100,10)\" href=\"#Extracting-Feature-Maps-from-a-Custom-PyTorch-Model\"><li style=\"font-size:medium\">Extracting Feature Maps from a Custom PyTorch Model</li></a>\n",
    "    </ol>\n",
    "    <a style=\"color:rgb(150,100,10)\" href=\"#Segmentation\"><li style=\"font-size:large;font-weight:bold\">Segmentation [75 points]</li></a>\n",
    "    <ol style=\"margin-bottom:15px\">\n",
    "        <a style=\"color:rgb(150,100,10)\" href=\"#Preparing-the-dataset-and-our-environment\"><li style=\"font-size:medium\">Preparing the dataset and our environment</li></a>\n",
    "        <a style=\"color:rgb(150,100,10)\" href=\"#Apply-UNet-for-segmentation\"><li style=\"font-size:medium\">Apply UNet for segmentation</li></a>\n",
    "        <a style=\"color:rgb(150,100,10)\" href=\"#Apply-attention-model-(open-Task)\"><li style=\"font-size:medium\">Apply attention model (Open Task)</li></a>\n",
    "    </ol>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(255,0,0)\">Please be aware that in this assignment we train and evaluate some models, which requires time. Thus, please avoid submitting your assignment at the last minute</h2>\n",
    "\n",
    "<h2 style=\"color:rgb(255,0,0)\">Any content unrelated to the assignment, such as images, Python code (functions or classes), additional packages, or libraries, etc., will result in a deduction of 50 points.</h2>\n",
    "\n",
    "<h1 style=\"color:rgb(255,0,0)\">Important Note</h1>\n",
    "\n",
    "<b>In this assignment, you may use the default parameters of any built-in function unless specified otherwise.</b>\n",
    "\n",
    "<b style=\"color:rgb(255,70,0)\">All cells must be executed; any unevaluated cell will not be considered, regardless of the correctness of the code.</b>\n",
    "\n",
    "<b style=\"color:rgb(255,70,0)\">All cells must be indexed; any non-indexed cell will not be considered.</b>\n",
    "\n",
    "<b style=\"color:rgb(255,70,0)\">Modifying the given code (even a single line), whether by replacing, changing, commenting, or removing, is strictly not allowed and will result in a loss of 50 points unless explicitly specified otherwise.</b>\n",
    "\n",
    "<b style=\"color:rgb(255,70,0)\">Redundant code, especially duplicating or copying the given code in different places, will result in a deduction of 25 points.</b>\n",
    "\n",
    "<b style=\"color:rgb(255,70,0)\">Importing the same packages multiple times in different cells will result in a deduction of 25 points.</b>\n",
    "\n",
    "<b style=\"color:rgb(255,70,0)\">All packages are imported in the first code cell. Any changes to this cell, including modifications, removals, or additions of packages or libraries, will result in a deduction of 50 points.</b>\n",
    "\n",
    "<b>If you have GPU resources available on your machine, please utilize them; otherwise, you can use your CPU.</b>\n",
    "\n",
    "<b>Always set the seed value to 123 unless specified otherwise.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all packages needed in this notebook.\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os, time, random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if not os.path.exists('resources'): raise TypeError(\"Please create the resources folder and include all required files\")\n",
    "print('All necessary packages are successfully installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Feature-Extraction\"></a><h2>Feature Extraction</h2>\n",
    "\n",
    "In this part, we are supposed to learn how to extract the feature maps from different models to see what the models are learning.\n",
    "\n",
    "<a name=\"Extracting-Feature-Maps-from-Pretrained-Torch-Models\"></a><h3>Extracting Feature Maps from Pretrained Torch Models</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1.1. [2 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Use a popular model from <i><b>torchvision</b></i> and extract its feature maps.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul> <li>Use <i><b>torchvision.models</b></i> to load <i><b>VGG16</b></i>.</li>\n",
    "<li>Set <i><b>pretrained=True</b></i>.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>The <i><b>VGG16</b></i> model is available in the <i><b>TorchVision</b></i> library.</ul>\n",
    "\n",
    "Documentation: <i><b>VGG16 — TorchVision</b></i> is [Here](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1.2. [2 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Extract the feature maps from the given model to observe what the model has learned.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul> \n",
    "<li>Use the <i><b>features_Extraction</b></i> function to extract the feature maps.</li>\n",
    "<li>Use the provided image <i><b>img_A2_1</b></i> located in the <i><b>resources folder</b></i>.</li>    \n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul> \n",
    "<li>You need to provide both the configured model and the target image to the <i><b>features_Extraction</b></i> function.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_Extraction(input_model, input_Image):\n",
    "    input_model.eval()\n",
    "    layer_to_hook = input_model.features[0]\n",
    "    \n",
    "    feature_maps = []\n",
    "    def hook_fn(module, input, output):\n",
    "        feature_maps.append(output.detach())\n",
    "    hook = layer_to_hook.register_forward_hook(hook_fn)\n",
    "    transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor() ])\n",
    "    img = Image.open(input_Image)\n",
    "    input_tensor = transform(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        _ = input_model(input_tensor)\n",
    "    hook.remove()\n",
    "    print(\"Shape of feature maps:\", feature_maps[0].shape)\n",
    "    \n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1.3. [1 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Visualize the feature maps to understand what the model learns\n",
    "\n",
    "<b>Task</b>\n",
    "<ul>\n",
    "<li>Use the function <i><b>visualize_Features</b></i> to visualize the feature maps.<i>\n",
    "<li>The number of feature maps displayed should be arranged in a grid of <i><b>3 rows</b></i> rows and <i><b>4 columns</b></i>.<i>   \n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>The <i><b>visualize_Features</b></i> function accepts three attributes: the <i><b>feature maps</b></i> themselves, and the <i><b>number of rows and columns</b></i> to display.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Features(feature_maps_toShow, nu_Rows=1, nu_Columns=1):\n",
    "    fm = feature_maps_toShow[0][0]\n",
    "    fig, ax = plt.subplots(nu_Rows, nu_Columns, figsize=(10,9))\n",
    "    for ix, axis in enumerate(ax.flat):\n",
    "        axis.set_title('Filter: '+str(ix))\n",
    "        axis.imshow(fm[ix].cpu())\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Extracting-Feature-Maps-from-a-Custom-PyTorch-Model<\"></a><h3>Extracting Feature Maps from a Custom PyTorch Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1.4. [4 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Prepare a custom model and extract the feature maps from it.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul>\n",
    "\n",
    "<li>Complete the cell below to create a custom model  as follows: \n",
    "<ol>\n",
    "  <li><i><b>Conv2d</b></i> layer with <i><b>3 input channels</b></i>, <i><b>16 output channels</b></i>, and a <i><b>kernel size of 5</b></i>.</li>\n",
    "  <li><i><b>ReLU</b></i> layer and <i><b>inplace is True</b></i>.</li>\n",
    "  <li><i><b>Conv2d</b></i> layer with <i><b>3 output channels</b></i>, and a <i><b>kernel size of 5</b></i>.</li>\n",
    "  <li><i><b>ReLU</b></i> layer and <i><b>inplace is True</b></i>.</li>\n",
    "  <li><i><b>MaxPool2d</b></i> layer and <i><b>kernel_size is 2</b></i></li>\n",
    "</ol>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>For simplicity, we create a few layers. However, you can extend the idea by creating more than these layers and extracting whichever maps you require after the training is done.</li>    \n",
    "</ul>\n",
    "\n",
    "Documentations are as follows: <i><b>Conv2d</b></i> is [Here](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) </li>, <i><b>ReLU</b></i> is [Here](https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html) </li>, and <i><b>MaxPool2d</b></i> is [Here](https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Custom_Ex14 = nn.Sequential(\n",
    "    \n",
    "    # code goes here\n",
    "   \n",
    "                 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1.5. [1 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Extract the feature map from a custom model.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul> \n",
    "<li>Use the <i><b>custom_Features_Extraction</b></i> function to extract the feature maps.</li>    \n",
    "<li>Use the same provided image <i><b>img_A2_1</b></i> from the <i><b>resources folder</b></i>.</li>\n",
    "<li>Pass the custom <i><b>model</b></i> created in <i><b>Ex 1.4</b></i> to the <i><b>custom_Features_Extraction</b></i> function.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul> \n",
    "<li>You need to provide both the custom model and our selected image as arguments to the <i><b>custom_Features_Extraction</b></i> function.</li>   \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_Features_Extraction(model_Custom_toExtract, ip_Img_CustomModel, id_intended=0):\n",
    "    feature_maps = {}\n",
    "    def get_feature_map(name):\n",
    "        def hook(model, input, output):\n",
    "            feature_maps[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    for idx, layer in enumerate(model_Custom_toExtract):\n",
    "        if id_intended ==idx:\n",
    "            layer.register_forward_hook(get_feature_map(f\"conv_{idx}\"))\n",
    "    \n",
    "    transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    img = Image.open(ip_Img_CustomModel)\n",
    "    input_tensor = transform(img).unsqueeze(0)\n",
    "    print(input_tensor.shape)\n",
    "    _ = model_Custom_toExtract(input_tensor)\n",
    "\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1.6. [15 Points]</b>\n",
    "\n",
    "<b>Question</b>\n",
    "<ul> \n",
    "<li>Q1) Let's assume that the custom model created in Ex. 1.4 will be trained on specific data and we set up the optimizer and the loss function. What steps are required in Ex. 1.4 to maintain spatial details and avoid vanishing gradients?</li>\n",
    "<li>Q2) Print both models (VGG16 and our custom model). What is the difference?</li>\n",
    "<li>Q3) Why can't we train our custom model for both feature extraction and classification tasks?</li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>Each question must be answered in <i><b>1~4 lines</b></i> maximum. Exceeding 4 lines may result in point deductions.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The explanation goes here:</b>\n",
    "\n",
    "A1) \n",
    "\n",
    "A2) \n",
    "\n",
    "A3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here ==>> for printing both models (VGG 16 and the custom model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Segmentation\"></a><h2>Segmentation</h2>\n",
    "\n",
    "In this part, we utilize one of the most popular techniques in image segmentation: <i><b>UNet</b></i>.\n",
    "\n",
    "<span style=\"color:rgb(200,100,10)\">More details about UNet model can be found [Here](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28)</span>.\n",
    "    \n",
    "\n",
    "<b>The main goal in this part can be broken down into three sub-goals as follows:</b>\n",
    "<ul>\n",
    "<li>Split the data into three parts (train, test, validation).</li>    \n",
    "<li>Train UNet to segment our object.</li>\n",
    "<li>Test our trained model using the unseen test subset.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color:rgb(255,0,0)\">Important Note</h3>\n",
    "<span style=\"color:rgb(125,0,0)\">We use one published version of UNet model. However, there are several <i><b>published models</b></i> that you can find.</span>\n",
    "\n",
    "We use a dataset containing one class, which is: <span style=\"color:rgb(200,0,200)\">Blood vessel pixels</span>. <span style=\"color:rgb(200,100,10)\">More details about the dataset can be found [Here](https://www.kaggle.com/datasets/abdallahwagih/retina-blood-vessel)</span>.\n",
    "\n",
    "<a name=\"Here is the example of crack segmentation\"></a><h3 style=\"color:rgb(100,0,200)\">Here is the example of Retina Blood segmentation from the site of [kaggle](https://www.kaggle.com/datasets/abdallahwagih/retina-blood-vessel)</h3>\n",
    "<img src=\"resources/Example_Retina-Blood-A2.jpg\" alt=\"Image not found!\"/>\n",
    "\n",
    "<h3 style=\"color:rgb(150,75,0)\">Let's work on our dataset and its preparation:</h3>\n",
    "<ul> <li><span style=\"color:rgb(180,40,40)\">The dataset, named <i><b>Retina-Blood-A2</b></i>, is zipped in <i><b>resources</b></i> folder and contains two subfolders: images and masks.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Preparing-the-dataset-and-our-environment\"></a><h3>Preparing the dataset and our environment</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.1. [1 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Read all images and their corresponding masks.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul>\n",
    "<li>Run the given function <i><b>segmDataset</b></i> to read all data and convert them to tensors.<i>\n",
    "<li>You need to provide the correct <i><b>path of the data</b></i>.</li> \n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>The provided <i><b>segmDataset</b></i> function already converts the images and their corresponding masks to tensors.</li>\n",
    "<li>You must <i><b>unzip</b></i> the dataset folder <i><b>Retina-Blood-A2</b></i> and place the unzipped folder inside the resources directory.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class segmDataset(Dataset):\n",
    "    def __init__(self, root_path, resize_IMG_MASK=320):  \n",
    "        self.root_path = root_path\n",
    "        self.resize_IMG_MASK = resize_IMG_MASK\n",
    "        self.images = sorted([self.root_path + \"/images/\" + i for i in os.listdir(self.root_path + \"/images/\" )])\n",
    "        self.masks = sorted([self.root_path + \"/masks/\" + i for i in os.listdir(self.root_path + \"/masks/\")])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.resize_IMG_MASK, self.resize_IMG_MASK)),\n",
    "            transforms.ToTensor(), ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
    "        mask = Image.open(self.masks[index]).convert(\"L\")\n",
    "        return self.transform(img), self.transform(mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.2. [3 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Prepare a custom dataset by splitting it into three subsets: training, validation, and test.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul>\n",
    "<li>Split the data into <i><b>train</b></i>, <i><b>validation</b></i>, and <i><b>test</b></i> subsets with the following ratios: <i><b>80%</b></i>, <i><b>10%</b></i>, and <i><b>10%</b></i>, respectively.<i>    \n",
    "<li>You must use the <i><b>random_split</b></i> function.</li> \n",
    "<li>Set the seed for the generator to <i><b>123</b></i> to ensure reproducible results.</li> \n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>The <i><b>random_split</b></i> function is part of the <i><b>torch.utils</b></i>.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.3. [3 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Prepare all subsets using the <i><b>DataLoader</b></i>. The <i><b>DataLoader</b></i> is essential for preparing the data in the proper format for training, validation, and testing.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul> <li>Use the <i><b>DataLoader</b></i> for training, validation, and testing subsets.</li>\n",
    "<li>Shuffle for training subset should be <i><b>True</b></i>. Other subsets are <i><b>False</b>.</li>\n",
    "<li>Set the batch size to <i><b>4</b></i>.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>The <i><b>DataLoader</b></i> function is part of the torch library.</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Apply-UNet-for-segmentation\"></a><h3>Apply UNet for segmentation</h3>\n",
    "<h4 style=\"color:rgb(150,75,0)\">Let's create the UNet model:</h4>\n",
    "<ul> <li><span style=\"color:rgb(10,10,10)\">The main model is in the class <i><b>UNet</b></i>; however, there are several auxiliary classes.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels))\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()        \n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        self.down4 = (Down(512, 1024 ))\n",
    "        self.up1 = (Up(1024, 512 ))\n",
    "        self.up2 = (Up(512, 256 ))\n",
    "        self.up3 = (Up(256, 128 ))\n",
    "        self.up4 = (Up(128, 64))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "        \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model_without_Att = UNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.4. [2 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Set up the optimizer and the loss criterion.\n",
    "\n",
    "\n",
    "<b>Task</b>\n",
    "<ul> <li>Use the <i><b>Adam optimizer</b></i> with learning rate of <i><b>2e-4</b></i>.</li>\n",
    "<li>Use <i><b>BCEWithLogitsLoss</b></i> as the loss function. </li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>Both functions <i><b>(Adam optimizer and BCEWithLogitsLoss)</b></i> are available in the torch library and can be easily invoked.</ul>\n",
    "\n",
    "More details about <i><b>BCEWithLogitsLoss</b></i> are available [here](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html).  \n",
    "Moreover, details about the <i><b>Adam</b></i> optimizer are available [here](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.5. [5 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Let's train our model using <i><b>data from the training subset</b><i>.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul><li>Use the provided <i><b>training_procedure</b></i> function to train the model parameters.</li>\n",
    "<li>You need to provide the defined <i><b>model</b></i>, <i><b>optimizer</b></i>, <i><b>loss function</b></i>, <i><b>training dataloader</b></i>, and <i><b>validation dataloader</b></i> to train the model.</li>\n",
    "<li>Use the attribute <i><b>name_of_Model</b></i> to specify the file name for the trained model parameters.</li>\n",
    "<li>Number of epochs is <i><b>10</b></i>.</li>\n",
    "</ul>\n",
    "    \n",
    "<b>Hint</b>\n",
    "<ul><li>The trained parameters are saved in the <i><b>resources</b></i> folder under your selected name with the <i><b>.pth</b></i> extension.</li>\n",
    "<li>The provided <i><b>accuracy</b></i> function calculates the accuracy, keep it unchanged.</li>\n",
    "<li>Run <i><b>training_procedure</b></i> and do not modify it.</li>\n",
    "<li><span style=\"color:rgb(180,10,10)\">Note that the training can take more than 6 minutes with a GPU and up to 3 hours without one (using a CPU). For example, my GPU completed the training in about 3–5 minutes, while my CPU took approximately 2~3 hours.</span></li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    pred = (torch.sigmoid(pred) >= 0.5).view(-1)\n",
    "    target = target.view(-1)\n",
    "    correct = (pred == target).sum().item()\n",
    "    total = target.numel()\n",
    "    acc = correct / total\n",
    "    return acc\n",
    "\n",
    "def training_procedure(model_to_Training, no_Epochs, optimiz_Func, loss_Func, trainData, validData, name_of_Model : str):\n",
    "    random.seed(123)    \n",
    "    device_in_Train = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    total_start_time = time.time()    \n",
    "    for epoch in tqdm( range(no_Epochs) ):\n",
    "        model_to_Training.train()\n",
    "        train_running_loss, train_running_acc = 0, 0\n",
    "        for idx, img_mask in enumerate(trainData):\n",
    "            img = img_mask[0].float().to(device_in_Train)\n",
    "            mask = img_mask[1].float().to(device_in_Train)\n",
    "            \n",
    "            y_pred = model_to_Training(img)\n",
    "    \n",
    "            loss = loss_Func(y_pred, mask)\n",
    "            train_running_loss += loss.item()\n",
    "        \n",
    "            train_running_acc += accuracy(y_pred, mask)\n",
    "        \n",
    "            # Compute gradient and do optimizer step\n",
    "            optimiz_Func.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiz_Func.step()\n",
    "\n",
    "        train_loss = train_running_loss / (idx + 1)\n",
    "        train_acc = train_running_acc / (idx + 1)\n",
    "        \n",
    "        model_to_Training.eval()\n",
    "        val_running_loss, val_running_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for idx, img_mask in enumerate((validData)):\n",
    "                img = img_mask[0].float().to(device_in_Train)\n",
    "                mask = img_mask[1].float().to(device_in_Train)\n",
    "                \n",
    "                y_pred = model_to_Training(img)\n",
    "                loss = loss_Func(y_pred, mask)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                val_running_acc += accuracy(y_pred, mask)\n",
    "\n",
    "            val_loss = val_running_loss / (idx + 1)\n",
    "            val_acc = val_running_acc / (idx + 1)\n",
    "\n",
    "            print(\"=\"*30)\n",
    "            print(f\"Train Loss EPOCH {epoch+1}: {train_loss:.4f}\")\n",
    "            print(f\"Valid Loss EPOCH {epoch+1}: {val_loss:.4f}\")\n",
    "            print(f\"Train accuracy EPOCH {epoch+1}: {train_acc:.4f}\")\n",
    "            print(f\"Valid accuracy EPOCH {epoch+1}: {val_acc:.4f}\")\n",
    "            print(\"=\"*30)\n",
    "\n",
    "        model_Saved_Location = f\"resources/{name_of_Model}.pth\"  \n",
    "        torch.save(model_to_Training.state_dict(), model_Saved_Location)\n",
    "        print(f\"EPOCH {epoch+1} is Done\")\n",
    "\n",
    "    total_time = (time.time() - total_start_time)\n",
    "    print('Total training time: {:.3f} hours'.format( ((total_time / 60)/60) ))\n",
    "    print('Training Done')\n",
    "\n",
    "    print(\"+\"*30)\n",
    "    print(f\"Training Loss {train_loss:.4f}\")  \n",
    "    print(f\"Validation Loss {val_loss:.4f}\")    \n",
    "    print(f\"Training accuracy {train_acc:.4f}\")  \n",
    "    print(f\"Validation accuracy {val_acc:.4f}\")\n",
    "    print(\"+\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.6. [3 Points]</b>\n",
    "\n",
    "<b>Goal: </b>Let's load the trained model and its parameters. Now, we can use the trained model to predict and segment <i><b>blood vessels</b><i> <i><b>using unseen data from the test subset</b><i>.\n",
    "\n",
    "\n",
    "<b>Task</b>\n",
    "<ul> <li>Call the model <i><b>UNet</b></i> again, and keep the model on your <i><b>CPU</b></i>.</li>\n",
    "<li>Use the <i><b>load_state_dict</b></i> function to load the trained model parameters.</li>\n",
    "<li>Set the <i><b>map_location</b></i> parameter to <i><b>CPU</b></i> when loading the model.</li></ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul><li>The trained parameters are located in the <i><b>resources</b></i> folder under your selected name with the <i><b>.pth</b></i> extension.</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.7. [3 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Now, we use the trained model to segment the <i><b>retina blood vessels</b><i>  based on <i><b>unseen data</b><i> from the <i><b>test subset</b><i>.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul> <li>Use the <i><b>test_procedure</b></i> function to segment the test data.</li>\n",
    "<li>Provide the <i><b>trained model</b></i> and the <i><b>test dataloader</b></i> as inputs.</li>\n",
    "<li>Display the first batch in a grid with <i><b>4</b></i> rows and <i><b>3</b></i> columns.</li>\n",
    "<li>The grid should display the following in order: the <i><b>RGB image</b></i>, the <i><b>ground truth</b></i>, and the <i><b>predicted segmented image</b></i>.</li>\n",
    "<li>The <i><b>colormap</b></i> of the <i><b>imshow</b></i> or <i><b>displayed images</b></i> should be <i><b>gray</b></i>.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>The <i><b>test_procedure</b></i> function returns three outputs: the RGB image, the ground truth, and the predicted segmented image.</li>\n",
    "<li>For simplicity, the <i><b>test_procedure</b></i> function returns only one batch</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_procedure(model_to_Test, testData):\n",
    "    with torch.no_grad():\n",
    "        for idx, img_mask in enumerate((testData)):\n",
    "            img = img_mask[0].to('cpu')\n",
    "            mask = img_mask[1].to('cpu')\n",
    "            pred_mask = torch.sigmoid( model_to_Test(img) )\n",
    "            if idx==0:\n",
    "                break\n",
    "        return img, mask, pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Apply-attention-model-(open-Task)\"></a><h3>Apply attention model (Open Task)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.8. [3 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Enhance the model by adding an attention mechanism.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul>\n",
    "<li>Use the attention model defined in the class <i><b>att_Model</b></i>.</li>\n",
    "<li>For simplicity, a class named <i><b>UNet_Att</b></i> has been created for you. Insert the attention model into the <i><b>UNet_Att</b></i> model.</li>\n",
    "</ul>\n",
    "    \n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>This is an open task. You may choose the locations and the number of insertions. It is also possible to insert the attention model in a single location.</li>\n",
    "<li><span style=\"color:rgb(180,10,10)\">You can modify the <i>reduction</i> attribute in the <i>att_Model</i> if needed.</span></li>\n",
    "<li><span style=\"color:rgb(190,48,48)\">Don’t change the name of the class <i><b>UNet_Att</b></i>.</span></li>\n",
    "<li><span style=\"color:rgb(180,10,10)\"><i><b>model_Att</b></i> is used as a different object name, keep it unchanged.</span></li>\n",
    "</ul>\n",
    "    \n",
    "<span style=\"color:rgb(200,100,10)\">For your reference, the paper link for this attention mechanism is: [Here](https://arxiv.org/pdf/1709.01507)</span> </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class att_Model(nn.Module):\n",
    "    def __init__(self, channel, reduction=1):\n",
    "        super(att_Model, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        b, c, w, h = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels))\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()        \n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet_Att(nn.Module):\n",
    "    \n",
    "    # code goes here\n",
    "    \n",
    "   \n",
    "        \n",
    "        \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model_Att = UNet_Att().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.9. [1 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Now, print the model after adding the attention mechanism.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul>\n",
    "<li>Print <i><b>model_Att</b></i>.</li>\n",
    "\n",
    "</ul>\n",
    "    \n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>Make sure that the cell containing <i><b>model_Att</b></i> has been executed.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.10. [8 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Train and test our new model using the attention mechanism.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul>\n",
    "<li>Use the provided <i><b>training_procedure</b></i> function to train the model <i><b>UNet_Att</b></i> created in <i><b>Exercise 2.8.</b></i> with the attention mechanism.</li>\n",
    "<li>Keep all attributes unchanged, including the number of epochs, optimizer, loss function, training dataloader, and validation dataloader.</li>\n",
    "<li>Within the <i><b>training_procedure</b></i> function, update the name of the newly trained model parameters to reflect the attention mechanism.</li> \n",
    "<li>Repeat <i><b>Exercises 2.6 and 2.7</b></i> using the model created in <i><b>Exercise 2.8</b></i>.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li><span style=\"color:rgb(180,10,10)\">Rewrite the <i>optimizer</i> to reflect the parameters of the newly trained model.</span></li>\n",
    "<li><span style=\"color:rgb(180,10,10)\">Use the <i><b>BCEWithLogitsLoss</b></i> variable as the loss function from Exercise 2.4 (don’t rewrite the loss function). Rewriting it will result in a deduction of points.</span></li>\n",
    "<li><span style=\"color:rgb(180,10,10)\">All necessary functions defined above must be called without copying them again. Otherwise, points will be deducted.</span></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.11. [10 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Train and test the model using your attention mechanism.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul>\n",
    "<li>Defined your attention model in the class <i><b>your_Att_Model</b></i>. <span style=\"color:rgb(180,10,10)\">You must select a different attention model than the one used in <i><b>Exercise 2.8</b></i>.</span></li>\n",
    "<li>For simplicity, a class named <i><b>your_UNet_Att</b></i> has also been created for you. Insert your defined attention model into the <i><b>your_UNet_Att</b></i> model.</li>\n",
    "<li>Print <i><b>your_UNet_Att</b></i>.</li>\n",
    "<li>Use the provided <i><b>training_procedure</b></i> function to train the <i><b>your_UNet_Att</b></i> model.</li>\n",
    "<li>Keep all attributes unchanged, including the number of epochs, optimizer, loss function, training dataloader, and validation dataloader.</li>\n",
    "<li>Within the <i><b>training_procedure</b></i> function, update the name of the newly trained model parameters to reflect the attention mechanism.</li> \n",
    "<li>Repeat <i><b>Exercises 2.6 and 2.7</b></i> using the <i><b>your_UNet_Att</b></i> model.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li><span style=\"color:rgb(190,48,48)\"><i><b>your_Model_Att</b></i> is used as a different object name, keep it unchanged.</span></li>\n",
    "<li><span style=\"color:rgb(190,48,48)\">Don’t change the name of the class <i><b>your_UNet_Att</b></i>.</span></li>\n",
    "<li><span style=\"color:rgb(190,48,48)\">Rewrite the <i>optimizer</i> to reflect the parameters of the newly trained model.</span></li>\n",
    "<li><span style=\"color:rgb(190,48,48)\">Use the <i><b>BCEWithLogitsLoss</b></i> variable as the loss function from Exercise 2.4 (don’t rewrite the loss function). Rewriting it will result in a deduction of points.</span></li>\n",
    "<li><span style=\"color:rgb(190,48,48)\">All necessary functions defined above must be called without copying them again. Otherwise, points will be deducted.</span></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class your_Att_Model(nn.Module):\n",
    "    \n",
    "    # code goes here\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels))\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()        \n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class your_UNet_Att(nn.Module):\n",
    "    \n",
    "    # code goes here\n",
    "   \n",
    "\n",
    "                \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "your_Model_Att = your_UNet_Att().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.12. [16 Points]</b>\n",
    "\n",
    "<b>Goal:</b> Train and test the model using provied and your attention mechanism.\n",
    "\n",
    "<b>Task</b>\n",
    "<ul>\n",
    "<li>insert <i><b>your attention model</b></i> in the <i><b>Exercises 2.11</b></i> and the provied model in the <i><b>Exercises 2.8</b></i> in the class <i><b>combined_UNet_Att</b></i>.</li>\n",
    "<li>For simplicity, a class named <i><b>combined_UNet_Att</b></i> has also been created for you. Insert both attention models into the <i><b>combined_UNet_Att</b></i> model.</li>\n",
    "<li>Print <i><b>combined_Model_Att</b></i>.</li>\n",
    "<li>Use the provided <i><b>training_procedure</b></i> function to train <i><b>combined_UNet_Att</b></i> model.</li>\n",
    "<li>Keep all attributes unchanged, including the number of epochs, optimizer, loss function, training dataloader, and validation dataloader.</li>\n",
    "<li>Within the <i><b>training_procedure</b></i> function, update the name of the newly trained model parameters to reflect the all attention models.</li> \n",
    "<li>Repeat <i><b>Exercises 2.6 and 2.7</b></i> using the <i><b>combined_UNet_Att</b></i> model.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li><span style=\"color:rgb(190,48,48)\"><i><b>combined_Model_Att</b></i> is used as a different object name, keep it unchanged.</span></li>\n",
    "<li><span style=\"color:rgb(190,48,48)\">Don’t change the name of the class <i><b>combined_UNet_Att</b></i>.</span></li>\n",
    "<li><span style=\"color:rgb(190,48,48)\">Rewrite the <i>optimizer</i> to reflect the parameters of the newly trained model.</span></li>\n",
    "<li><span style=\"color:rgb(190,48,48)\">Use the <i><b>BCEWithLogitsLoss</b></i> variable as the loss function from Exercise 2.4 (don’t rewrite the loss function). Rewriting it will result in a deduction of points.</span></li>\n",
    "<li><span style=\"color:rgb(190,48,48)\">All necessary functions defined above must be called without copying them again. Otherwise, points will be deducted.</span></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class your_Att_Model(nn.Module):\n",
    "    \n",
    "    # code goes here ==>> should be same as Exercise 2.11.\n",
    "    \n",
    "    \n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "class att_Model(nn.Module):\n",
    "    def __init__(self, channel, reduction=1):\n",
    "        super(att_Model, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        b, c, w, h = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels))\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()        \n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class combined_UNet_Att(nn.Module):\n",
    "    \n",
    "    # code goes here\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "combined_Model_Att = combined_UNet_Att().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.13. [12 Points]</b>\n",
    "\n",
    "<b>Question</b>\n",
    "<ul> \n",
    "\n",
    "<li>Q1) How can we differentiate between different vessels (the main branch and sub-branches)?</li>\n",
    "<li>Q2) What is the problem in our blood vessel data when we differentiate between the main branch and sub-branches?</li>\n",
    "<li>Q3) What would you notice if we apply Grad-CAM?, and why?</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>Each question must be answered in <i><b>1~4 lines</b></i> maximum. Exceeding 4 lines may result in point deductions.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The explanation goes here:</b>\n",
    "\n",
    "A1) \n",
    "\n",
    "A2) \n",
    "\n",
    "A3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2.14. [5 Points]</b>\n",
    "\n",
    "<b>Question</b>\n",
    "<ul> \n",
    "\n",
    "<li>Q1) Can you upload the Grad-CAM visualizations of our trained models based on the last layers, for all versions (without attention and with attention)?</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint</b>\n",
    "<ul>\n",
    "<li>You can use Pillow to read the images and imshow from Matplotlib to display them.</li>\n",
    "<li>You can obtain the Grad-CAM image using any available online resource.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1067ee20f23cf75b48768bdb5f7ec1d4c21e1831c972d070ed1f98bb55bb7e57"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
